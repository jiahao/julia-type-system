\section{Introduction}
Programming is becoming a growing part of the work flow of those working in the physical scientists. [Say something comparing the number of type of programmers in some previous decade to now?] These programmers have demonstrated that they often have needs and interests different from what existing languages were designed for.

In this paper, we focus on the phenomenon of how dynamically typed languages such as Python, Matlab, R, and Perl have become popular for scientific programming. Dynamic languages features facilitate writing certain kinds of code, use cases for which occur in various technical computing applications.
Holkner~\etal's analysis of Python programs~\cite{Holkner2009} revealed the use of dynamic
features across most applications, occurring mostly at startup for data I/O, but
also throughout the entire program's lifetime, particularly in programs
requiring user interactivity. Such use cases certainly appear in technical
computing applications such as data visualization and data processing scripts.
Richards~\etal's analysis of JavaScript programs~\cite{Richards2010} noted that dynamic
features of JavaScript were commonly use to extend behaviors of existing types
like arrays. Such use cases are also prevalent in technical computing
applications, to imbue existing types with nonstandard behaviors that are
nonetheless useful for domain areas.

An issue that arises with dynamically typed languages is performance.
Code written in these languages is difficult to execute efficiently~\cite{Joisha2001,Joisha2006,Seljebotn2009}.
While it is possible
to greatly accelerate dynamic languages with various techniques, for
technical computing the problem does not stop there. These systems
crucially depend on large libraries of low-level code that provide array
computing kernels (e.g. matrix multiplication and other linear algebraic
operations). Developing these libraries is a challenge, requiring a range of
techniques including templates, code generation, and manual code
specialization. To achieve performance users end up having to transcribe their prototyped codes into a lower level static language, resulting in duplicated effort and higher maintenance costs. [Talk about how this happens in Cython.]

We have designed the Julia programming language~\cite{Bezanson2012, Bezanson2014} allows the programmer to combine dynamic types with static method dispatch. We identify method dispatch as one of the key bottlenecks in scientific programming. As a solution we present \emph{gradual dispatch}, a typing scheme that allows the programmer to optionally provide static annotations for types. In this paper, we describe Julia's multiple dispatch type system, the annotation language, and the data flow algorithm for statically resolving method dispatch. By analyzing popular packages written in Julia, we demonstrate that 1) people take advantage of Julia's type inference algorithm, 2) the Julia compiler can statically resolve X\% of method dispatch in Julia programs, and 3) static dispatch in Julia provides performance gains because it enables inlining. [Also show that statically resolving dispatch provides speedups?]

%We have designed the Julia programming language~\footnote{Julia is free and open-source software available from \url{http://julialang.org} under the MIT license. In this paper, we describe the behavior of the v0.4.0-dev+3270 point release.} to be a high-level technical language that also has good performance.
%The key insight in the design of the type system is that it supports \emph{type tags}, optional static annotations that can be used for both performance and reasoning. [Say some high-level sentences about what type tags are.]

%It turns out that everyone really wants static types for performance anyway. Julia gives everyone what they want because it has a type system based on dynamic dispatch that the programmer can gradually type for static dispatch. As a result, most of Julia programs dispatch statically. The static dispatch arises in Y% of X Julia packages and provides [concrete amount] of performance speedup over code that is fully dynamically dispatched. (Say some sentences that talks about how to consider Julia performance w.r.t. other similar languages to make us seem more legit.)

Our main contributions are as follows:
\begin{itemize}
\item We present the Julia language and its multiple dispatch semantics.
\item We describe the data flow algorithm for resolving Julia types statically.
\item We analyze X Julia packages and show that Y% of type dispatch can be resolved statically.
\item (Talk about how much static dispatch speeds things up?)
\item (Talk about how much of the code actually gets annotated?)
\end{itemize}

%In this paper, we describe the algorithm for taking advantage of these static annotations. We describe how the static type resolution provides \TODO{how many x} speedup over code without annotations in a set of representative benchmarks. In addition, we describe how the Julia standard library, used by \TODO{how many} users, takes advantage of Julia's flexible dispatch mechanism for extension and reuse. When combined with an extensive base library for parallel execution and numerical analysis, Julia provides a convenient environment for rapid prototyping of new scientific computing code that can be deployed often within a factor of two of the speed of native, hand-written C code.

%
%% which together naturally express the polymorphism inherent in technical
%% computing. Julia's implementation also features a just-in-time compiler which
%% performs extensive static optimizations such as:

%% \begin{itemize}
%%  \item Automatic type inference, which minimizes the overhead of dynamic
%%        multiple dispatch and largely eliminates the need for explicit
%%        type annotations in function bodies.
%%  \item Tuple elimination
%%  \item Function inlining
%%  \item A raft of compiler optimizations such as dead code elimination that
%%        are provided by the LLVM library~\cite{Lattner2004}.
%% \end{itemize}

%% This paper aims to explain how Julia's language constructs and compiler
%% optimizations conspire to provide expressiveness without compromising
%% performance, by carefully designing a language that is purposely amenable to
%% static analysis. When combined with an extensive base library for parallel
%% execution and numerical analysis, Julia provides a convenient environment for
%% rapid prototyping of new analytics which can also be deployed performantly,
%% often within a factor of two of the speed of native, hand-written C code.

%While multiple dispatch has been extensively studied as an object-oriented
%paradigm in the past, it has not been widely adopted in practice~\cite{Muschevici:2008}.
%With Julia, a growing number of scientific programmers are using multiple dispatch
%extensively. The key property of multiple
%dispatch for this domain seems to be that it allows programs (particularly libraries)
%to be written incrementally, in small pieces, where each piece has a method signature
%specifying \emph{declaratively} how it fits into the whole.
%More sophisticated types ease this process, since they allow new definitions to be added smoothly in more situations.

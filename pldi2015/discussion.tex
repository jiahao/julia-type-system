\section{Discussion}

\subsection{Why is dynamic multiple dispatch useful for numerical computing?}

Dynamic multiple dispatch is valuable for writing generic numeric code which
handles the broadest possible set of inputs while at the same time making use
of handcrafted performant kernels when they are available. An illustration of
this principle can be seen in Julia's linear algebra routines. Many systems
provide optimized implementations of standard libraries such as BLAS (Basic
Linear Algebra Subprograms) and LAPACK (Linear Algebra PACKage) which provide
standard functionality such as matrix-matrix multiplication and spectral
decompositions for computing eigenvalues and eigenvectors: Implementations such
as ATLAS, GotoBLAS and OpenBLAS provide performant kernels customized for
individual microarchitectures that can reach a significant fraction of
theoretical peak FLOPS. 

There are a few limitations, however. First, these routines are defined only for
hardware-representable floating point numbers. Second, because these libraries are written in Fortran, with no notion of polymorphic dispatch, we have to deal with an awkward nomenclature 
%XXX ^^^ Also, ^^^ These libraries are written
in Fortran 77 and have an awkward nomenclature inherited from old-style
Fortran. The procedures have short names (six letters long in the earliest
versions) and the first letter of each proc (Basic Linear Algebra Subprograms)
If the matrix is composed of floating point types that BLAS and
LAPACK routines can handle, the function call is dispatched onto an
appropriate BLAS/LAPACK routine. Yet if the matrix contains other
numeric types, such as rational numbers or quaternions, Julia provides
generic methods which handle the computations using standard textbook
algorithms. In this way, multiple dispatch allows the widest possible
set of inputs while not giving up the ability to use handcrafted
performant kernels when they are available.

\TODO{What is the motivating problem for this? Is it that we get matrices where the different types are unknown?
Can we show specific code for this?
How would this look in a different language?
need more context - what is blas, lapack, why need to support custom user element types?
widest possible set of inputs -> explain why there is need to support custom numeric types? not necessarily mappable to hw floating point types
reuse same code, using just dispatch to handle computation with generic fallback or hand-optimized numeric kernels in LAPACK/BLAS
as opposed to having differently named functions, e.g. s/d/z/c/gelsy (xgelsy) - make clear how dispatch allows for overloading}

The ability to perform multiple dispatch dynamically is valuable
when you can specialize on certain algorithms but you can only
determine which algorithm to dispatch on at runtime. This is useful
for linear algebraic applications where certain properties of an input
matrix are difficult to determine \textit{a priori}, such as matrix
symmetries like Hermiticity, or spectral properties like positive
definiteness. The matrix square root function (\code{sqrtm}) uses
runtime multiple dispatch to check if the input matrix is symmetric
(for real element types) or Hermitian (for complex element types). If
the matrix is symmetric/Hermitian, \code{sqrtm} dispatches on a
specialized method to compute the principal matrix square root using
the eigenvalues and eigenvectors, otherwise a different method is
dispatched upon which computes and uses the Schur factorization
instead. The \code{factorize} function in Julia is another, more
extreme example of how dynamic multiple dispatch can be used to detect
matrix symmetries and structure (e.g. if a matrix is diagonal,
symmetric tridiagonal, upper triangular, etc.) and compute appropriate
matrix factorizations that best take advantage of the detected matrix
structure.

\TODO{show code examples}

\TODO{Other possible examples from Distributions.jl: QQ plots, posterior function}

\subsection{External dispatch with dynamic multimethods}

One thing you can do with dynamic multiple dispatch is external dispatch,
i.e.\ to be able to add new methods to an existing type rather than
specifying all the possible methods upfront. This is used to good effect
in Julia packages to extend functionality of the base library. For example,
the package \package{Color.jl} for manipulating color spaces extends the
\code{writemime} base function, which writes an object as a specified
MIME type\cite{mimerfc} to an I/O stream. \code{Color.jl} extends
\code{writemime} with new methods that draw color swatches.

\TODO{Here, we could put a figure showing writemime being called on a base
Julia object, writemime(::Color), and writemime(::VectorColor) objects.
Possibly take a screenshot in IJulia.}

External dispatch also facilitates interaction between Julia packages--- 
colors can be rendered using the \package{Cairo.jl} package,
which allows rendering to a Cairo backend~\cite{cairographics}.
When using Cairo to render an object in a given color, \code{Cairo.jl}
doesn't need to know anything about the underlying color space; it just
needs to call \code{convert} methods provided by \code{Color.jl}.
Julia's \code{convert} protocol is the ``narrow waist'' that allows
packages to allow using all kinds of color spaces naturally, without
needing to know anything specifically about them.

External dispatch can be performed in other languages like C++. However,
C++ only allows static external dispatch, which severely limits its utility.
Furthermore, external dispatch in C++ requires virtual methods and visitor
patterns\cite{designpatterns} to work around the absence of explicit support
for external dispatch.

\TODO{Talk about the psychological implications of types.}

%-----------------------------------------------------------------------------
%
%               Template for sigplanconf LaTeX Class
%
% Name:         sigplanconf-template.tex
%
% Purpose:      A template for sigplanconf.cls, which is a LaTeX 2e class
%               file for SIGPLAN conference proceedings.
%
% Author:       Paul C. Anagnostopoulos
%               Windfall Software
%               978 371-2316
%               paul@windfall.com
%
% Created:      15 February 2005
%
%-----------------------------------------------------------------------------

\documentclass[pldi]{sigplanconf-pldi15}

%
% the following standard packages may be helpful, but are not required
%
%\usepackage{SIunits}            % typset units correctly
%\usepackage{courier}            % standard fixed width font
%\usepackage[scaled]{helvet} % see www.ctan.org/get/macros/latex/required/psnfss/psnfss2e.pdf
\usepackage{url}                  % format URLs
%\usepackage{listings}          % format code
%\usepackage{enumitem}      % adjust spacing in enums
\usepackage[colorlinks=true,allcolors=purple,breaklinks,draft=false]{hyperref}   % hyperlinks, including DOIs and URLs in bibliography
% known bug: http://tex.stackexchange.com/questions/1522/pdfendlink-ended-up-in-different-nesting-level-than-pdfstartlink
%\newcommand{\doi}[1]{doi:~\href{http://dx.doi.org/#1}{\Hurl{#1}}}   % print a hyperlinked DOI

%\usepackage{pslatex}
\usepackage{todonotes} %TODO Remove for final submission
\usepackage{minted}

\input{macros}
\begin{document}

\conferenceinfo{PLDI '15}{June 12---20, 2015, Portland, OR, USA.}
\copyrightyear{2015}
%\copyrightdata{}

%\titlebanner{banner above paper title}        % These are ignored unless
\preprintfooter{DRAFT - do not distribute}   % 'preprint' option specified.

\title{Efficient Multiple Dispatch in Julia}
%\subtitle{Subtitle Text, if any}

\authorinfo{Jeff Bezanson \and Jake Bolewski \and Jiahao Chen \and Stefan Karpinski \and Jean Yang \and Alan Edelman}
	{Massachusetts Institute of Technology}
	{bezanson@mit.edu, jake.bolewski@gmail.com, jiahao@mit.edu, stefan@karpinski.org, jeanyang@mit.edu, edelman@mit.edu}

\maketitle

\begin{abstract}
The goal of scientific programs is often to create experiments rather than to
build robust systems. Because scientific programs frequently involve operating
over reified data values with difficult-to-predict types, it is often easier
for scientific programmers to use dynamically typed languages. Unfortunately,
dynamic types often make it difficult to execute code efficiently. In this
paper, we describe the type system and multiple dispatch mechanism for the
Julia programming language, specialized for scientific computing. Julia has a
novel semantics based on efficient multiple dispatch. Julia combines
programmer-specified type tags with a type inference engine to statically
optimize method dispatch. Julia is dynamically typed in that the compiler does
not statically reject ill-typed programs, but Julia is designed to optimize
static type and dispatch inference. We describe the Julia language and type
system and our implementation of type inference and multiple dispatch
mechanisms for Julia. To demonstrate the relevance and potential impact of
designing a language based around multiple dispatch, we also describe the
manifestations of multiple dispatch in Julia's standard library.
\end{abstract}

\category{D.3.3}{PROGRAMMING LANGUAGES}{Language Constructs and Features}

\terms Languages, Multiple dispatch, Multimethods

%\keywords
%Language design, run-time system



\section{Introduction}

%TODO These paragraph markings are structural only; take them out for final submission 
\paragraph{Why dynamically typed languages?} 
Dynamically typed languages are popular when computing on data of uncertain
formats or with uncertain outcomes.  Suppose you have a ten-terabyte dataset.
You want to find the parts of it that are useful.  The first step is to
summarize the data set.  There are various tools to summarize the data set, but
it is usually not clear which of the tools will produce useful summaries.  This
process is one of rapid iteration: the programmer must keep trying summary
toolchains until there is one that works.

\paragraph{Dynamic languages are good for rapid iteration and scientific experimentation}
Dynamically typed languages are often better suited for this rapid iteration
process.  In this class of programs, the programming process is one of running
experiments rather than one of building robust systems.  This is the nature of
many scientific programs: programs operating over data sets created by
scientific experiments.  Scientific programs frequently involve operating over
reified data values whose types are difficult to predict.  Because it is not
always clear what the properties of the data are going to be, it is not clear
what the types of the values and which functions to call.  Writing this code in
statically typed languages involves extensive case statements on types and
typed enumeration of the possible different cases.  The programmer can avoid
this in dynamic languages.
\TODO{Point: static languages introduce straitjackets that constrain the pliability and reusability of code.}

\paragraph{Classes are awkward to express technical computation}
Binary operations are awkward to handle with class-based object-orientedness.
The correct implementation of *(a, b) ought to depend on the type of both a and
b; however, most conventional OO languages only allow dispatch on the type of
a. Thus to implement Matrix.*(b) the method has to distinguish between b being
a matrix, vector or scalar, and implement the appropriate matrix-matrix,
matrix-vector or matrix-scalar product appropriately.. In contrast, a language
supporting multiple dispatch and multimethods allows for multiple methods
*(a::Matrix, b::Matrix), *(a::Matrix, b::Vector) and *(a::Matrix, b::Number) to
be defined and for method dispatch to depend on the types of both arguments.

\paragraph{Technical computing can use multiple dispatch for > 2 arguments}
Multiple dispatch becomes even more powerful when dispatch on multiple
arguments is supported. In principle we can even take this a step further and
allow support of *(a::Matrix, b::Matrix, c::Matrix, ...) where a single
multiplication method will handle an arbitrary chain of matrix-matrix products.
Neglecting concerns over exact associativity in floating-point operations, we
can take advantage of associativity to evaluate the matrix product in
computationally efficient ways. For example, (100 x 5) x (5 x 30) x (30 x 1)
results in a (100 x 1) matrix. If you do it left to right you construct a 100 x
30 matrix intermediate, but if you do it right to left you construct a 5 x 1
intermediate. Associativity lets you play games where you can construct an
optimal chain of multiplications which minimize memory accesses and scalar
computations.
 

\paragraph{Multiple dispatch is good}
We present a solution based around efficient multiple dispatch.  Much of the
overhead in dynamic languages comes from dynamic lookups of function dispatch.
\TODO{Is this actually true?} We present a programming language Julia where the
key insight is designing a dispatch mechanism that allows for as much static
resolution as possible.  Julia programs are organized around multiple dispatch:
programs overload functions for different combinations of argument types.  The
key ingredients in Julia are multi-methods and a static data flow algorithm for
type inference.  Julia's multi-methods are designed around static analysis to
resolve programmer-specific type tags as much as possible.  Julia is
dynamically typed in that the compiler does not statically reject ill-typed
programs, but Julia is designed to optimize static type and dispatch inference.
\TODO{Explain type tags as dynamic tags with runtime coercion, cross-reference literature on dynamically typed lambda calculi.}
\TODO{Contextualize use of multiple dispatch}

\paragraph{Explain point of paper}
In this paper, we describe the Julia language and type system and our
implementation of type inference and multiple dispatch mechanisms for Julia.
Julia is a high-performance dynamic language with a compiler, distributed
parallel execution, numerical accuracy, and an extensive mathematical function
library. Julia has a dynamic semantics based on efficient multiple dispatch.
Julia combines programmer-specified type tags with a type inference engine to
statically optimize method dispatch. To demonstrate the relevance and potential
impact of designing a language based around multiple dispatch, we also describe
the manifestations of multiple dispatch in Julia's standard library.

The main contributions of this paper are to describe:

\begin{itemize}
	\item the multiple dispatch semantics of the Julia language.
	\item our algorithms for statically resolving method dispatches when information is available.
	\item the uses of multiple dispatch in Julia's standard library.
	\item the performance of Julia benchmarks. \TODO{Compare to other numbers.}
\end{itemize}

\paragraph{Limitations of class-based dispatch for numerics}
Arguably, class-based dispatch is not natural for numeric computations.  It's
difficult to know upfront all the possible things you want to do to a floating
point number.  In languages like C++, you can extend existing classes with new
methods, but it's difficult.  Need to have virtual methods and invoke template
specialization with them.  Oftentimes you also need runtime code to decide what
kind of object to create.

Static languages like Haskell have typeclasses\cite{typeclass}, but you'll need
to anticipate all the necessary use cases ahead of time because everything is
resolved statically and also know all the cases you'll be in at compile time.
Dynamic languages provide only dynamic bindings, which means that programmers
don't have to reason about the differences between static and dynamic
semantics, and dynamic binding is more general anyway.

\paragraph{Dynamic languages, realism and empiricism}
Arguably, dynamic languages are more in concord with the empirical mode of
scientific inquiry that is familiar to users of technical computing, many of
whom are also scientists and engineers.  Dynamic languages embody a realist
philosophy: programs are not checked for correctness, but are executed until
termination or when a runtime error is thrown.  The focus is to make sense out
of whatever program a user may write.  In contrast, static languages focus on
formal correctness, validating programming on the basis of satisfying
constraints imposed by static analyses.  Furthermore, these formal systems tend
to concern themselves with only the interface to data types, not with their
internal representation.  Consequently, the formal logic of program correctness
does nothing for user concerns about performance, since they abstract away
memory layout details which is crucial for understanding the impact of hardware
factors such as bus latency and bandwidth.

\paragraph{Empirical philosophy facilitates experimentaion}
As a result of the more liberal attitude taken toward program validation,
dynamic languages lend naturally to rapid iteration through many prototypes and
versions of computer programs.  This sort of experimentation in writing
programs comes naturally to technical computing users, who often have to write
programs without any idea of what the final result ought to look like.  Dynamic
languages offer a natural expression for these use cases that lack formal
specifications, by not imposing upon users the burden of writing formally
correct programs, allowing them to focus instead on expressing how to do
practical computations.  In other words, a scientist will often run a program
in order to find out what it does --- \emph{proving} anything about what it
does would be premature, and an unwelcome distraction.  Of course some such
proofs are innocuous, for example that the program will return a floating-point
number.  However such proofs are also of no particular interest in the
application domain.

\paragraph{Multiple dispatch allows dispatch on new types and new functions at the same time}
\TODO{Maybe this should be "limitations of class-based dispatch" instead}
The needs of technical computing can exceed the abilities of most dispatch
systems.  In particular, it is usually not possible to have new behaviors that
intermix new types and new functions.  One group of languages allows you to
have new types but their dispatch upon existing functions cannot be defined.
Subtyping is the usual paradigm here, but it is closed because it assumes
you've covered all the cases explicitly.  Object oriented programming can be
seen as a solution around this problem, but in pure OO, objects have only
identity and have no interface protocol.  Classes are a mechanism for
implementing message sends, defining actions \textit{upon} an object.  The
other group of languages allows you to have new functions but not for existing
types.  Haskell typeclasses~\cite{typeclass} are a fixed collection of
interfaces; while additional functions can be defined, only the functions that
form part of the existing interface can interact with an existing type.

\paragraph{Fewer language constructs for user simplicity}
The original motivation in Julia for having types as values was to simplify the
language for technical computing users.  The current design came about from
considering the desire for users to simplify code dealing with parametrically
polymorphic types, i.e.\ be able to write things like \code{Array} as a synonym
for \code{Array\{T\}}.  Such synonyms are valuable for composability when
writing methods that are agnostic about the type parameter \code{T}.  Methods
that cared about \code{T} can be defined with a type signature that explicitly
mentions \code{T} and methods that did not care about \code{T} can have a type
signature that left it out.  The ability to leave out type parameters in
function signatures contrasts with other languages \TODO{like which} which
require explicit specification of all type parameters, resulting in users
having to write redundant code blocks whose sole purpose is handle the nuisance
parameter.  In contrast, the value proposition in Julia is that having types as
values, a collapsed kind hierarchy (i.e.\ making no distinction between types
and kinds (meta-types) and meta-kinds etc.), plus being able to reason about
types dynamically, affords users a way to harmonize the use of types and
pattern matching language constructions that are common in other languages.



\section{Motivating Example}

\TODO{A good motivating example for types is $2^{64}$}

\subsection{Why is dynamic multiple dispatch useful for numerical computing?}
Dynamic multiple dispatch is valuable for writing generic numeric code which
handles the broadest possible set of inputs while at the same time making use
of handcrafted performant kernels when they are available.  An illustration of
this principle can be seen in Julia's linear algebra routines.  Many systems
provide optimized implementations of standard libraries such as BLAS (Basic
Linear Algebra Subprograms) and LAPACK (Linear Algebra PACKage) which provide
standard functionality such as matrix-matrix multiplication and spectral
decompositions for computing eigenvalues and eigenvectors.  Implementations
such as ATLAS, GotoBLAS and OpenBLAS provide performant kernels customized for
individual microarchitectures that can reach a significant fraction of
theoretical peak FLOPS.

\subsection{Limitations of Fortran libraries}
There are a few limitations, however.  Chief among these is the cognitive
burden for users to learn how these routines are named and figuring out which
routine performs the desired computations.  These libraries, like many
scientific codes, are written in Fortran, with no notion of polymorphic
dispatch.  Most of these routines have six letter names in line with
traditional Fortran 77 subroutine nomenclature.

\begin{enumerate}
\item The first letter of each routine name describes the floating point type (e.g.\ single precision real, double precision real, single precision complex and double precision complex).

\item The next two letters is a code for the type of matrix (e.g.\ \code{GE} for general, \code{ST} for symmetric tridiagonal).

\item The last few letters describe the type of computation to carry out (e.g.\ \code{SV} for linear solver, \code{EIN} for eigenvalues and eigenvalues).
\end{enumerate}

\paragraph{No polymorphism in Fortran = verbose and redundant}
The lack of polymorphism produces a proliferation of routines with a large
amount of redundant code. Furthermore, it is very difficult to extend these
libraries with new functionality, as many redundant routines have to be written
even something as fundamental as adding a new level of floating-point
precision. \TODO{e.g. QUADPACK is its own thing. Things get even worse with mixed-precision.}


\paragraph{Polymorphism with generic functions = expressive overloading}
In contrast, Julia's system of generic functions with polymorphic dispatch
system allows a compact factorization of the namespace of linear algebra
routines.  Instead of a routine name like \code{DSTEIN} (double precision
symmetric tridiagonal eigenproblem solver), the \code{LinAlg} module in base
Julia introduces the function \code{eig} which solves eigensystems, and allows
parametric method signatures like
\code{eig\{T<:BlasFloat\}(M::SymTridiagonal\{T\})} which allow specialized
methods to be defined for both the matrix type (\code{SymTridiagonal},
symmetric tridiagonal) and its element type (\code{BlasFloat} is a union type
\code{Union(Float32, Float64, Complex\{Float32\}, Complex\{Float64\})}, and so
\code{T<:BlasFloat} specifies a template that defines four separate,
specialized methods.  The \code{LinAlg.LAPACK} module also uses Julia's
metaprogramming capabilities to map Julia function calls onto their LAPACK
equivalents.  In this way, multiple dispatch allows a handful of overloaded
functions to accept the widest possible set of inputs while not giving up the
ability to use handcrafted performant kernels when they are available.

\paragraph{Extensible generic functions allows for fast specalizations and generic fallbacks to coexist}
The ability to add new specialized methods to existing generic functions allows
new functionality to be added seamlessly. For example, Julia provides generic
fallback routines to do matrix computations over arbitrary fields of element
types \code{T}, providing the ability to compute on numeric types which are not
mappable to hardware floating point types. This can be useful to perform matrix
computations in exact rational arithmetic or software--emulated higher
precision floating point arithmetic to verify the implementations of algorithms
or to detect the possibility of numerical instability associated with roundoff
errors. Oftentimes it is possible to write a single kernel that will work
independently of the underlying field \code{T}, avoiding the need to write
redundant code.

\paragraph{Dynamic multiple dispatch is useful for polyalgorithms requiring runtime information}
\TODO{Emphasize that the algorithm used is value-dependent} 
The ability to perform multiple dispatch dynamically is valuable when you can
specialize on certain algorithms but you can only determine which algorithm to
dispatch on at runtime. This is useful for linear algebraic applications where
certain properties of an input matrix are difficult to determine \textit{a
priori}, such as matrix symmetries like Hermiticity, or spectral properties
like positive definiteness. For example the matrix square root function
(\code{sqrtm}) includes a runtime check to detect if the input matrix is
symmetric (for real element types) or Hermitian (for complex element types). If
the matrix is symmetric/Hermitian, \code{sqrtm} dispatches on a faster,
specialized method to compute the principal matrix square root using the
eigenvalues and eigenvectors, otherwise a different method is dispatched upon
which computes and uses the Schur factorization instead.

\paragraph{sqrtm}
The generality afforded by dynamic dispatch allows for runtime dispatch onto
optimized kernels depending on the input matrix. For example, the
\code{sqrtm\{M::Matrix\}} function for computing matrix square roots contains a
runtime check to detect if the input matrix is symmetric. If it is, then the
computation is dispatched onto a \code{sqrtm\{S::Symmetric\}} method which
computes more efficiently than the general case. Of course if the user knows at
the time of writing the code he or she could call the specialized method
directly, but the ability to do dynamic dispatch allows the same performant
kernel to be called even in situations where the user does not know about the
special symmetry of a particular input matrix, or perhaps may not even be aware
of specialized algorithms that can be enabled by such information that can be
deduced at runtime.

\paragraph{factorize}
The \code{factorize} function in Julia is another, more extreme example of how
dynamic multiple dispatch can be used to detect matrix symmetries and structure
(e.g.\ if a matrix is diagonal, symmetric tridiagonal, upper triangular, etc.)
and compute appropriate matrix factorizations that best take advantage of the
detected matrix properties. The output is a matrix factorization object whose
specific type is only defined at runtime depending on the specific matrix
properties that are detected at runtime. For example, calling \code{factorize}
on a symmetric positive definite matrix returns a \code{Cholesky} type
representing a Cholesky factorization, whereas a symmetric indefinite matrix
would yield a \code{BunchKaufman} type encoding a Bunch--Kaufman factorization
instead. Positive definiteness is a good example of a matrix property that can
be difficult to determine \textit{a priori} and the fastest way to detect it
would be to attempt to compute a matrix factorization that succeeds only on
postiive definite matrices (such as the Cholesky decomposition), and fallback
to returning a different type if the computation failed.

\paragraph{User interpretation of generic functions}
Generic functions express a common action or computation, which may in
practice be performed in many different ways depending on the inputs. For
example, one may wish to compute the mean of a given statistical distribution.
In Julia, the computation can be expressed simply as dispatch onto the method
\code{mean(D::Distribution)}, with a different method for \code{mean} being
defined for different \code{Distribution}s. In contrast, much scientific code
is written in languages that lack generic functions, resulting in users having
to keep track of a large family of very closely related functions which
together can be thought of as specialized mthods for a polymorphic function.
Table~\ref{statsfunctions} shows a family of closely related functions in the R
programming language and their Julia counterparts in the
\package{Distributions.jl} package. Julia provides a common functional interface
for each type of statistical distribution, with generic functions like
\code{pdf} to compute the probability density function, \code{cdf} for the
cumulative density function, \code{quantile} to compute quantiles, and
\code{rand} for sampling random variates. In contrast, each distribution in R
must have its own associated family of functions prefixed by \code{d},
\code{p}, \code{q} and \code{r} respectively, and the number of functions grows
by both the number of distributions and the number of desired computations on
them.

\code{pdf}
\begin{table}
{\tiny
\begin{tabular}{l || l | l | l | l}
  \textbf{\code{D}}                & \textbf{\code{pdf(D)}}    & \textbf{\code{cdf(D)}}    &
        \textbf{\code{quantile(D)}} & \textbf{\code{rand(D)}}  \\ \hline\hline
  \textbf{\code{Beta}}             & \code{dbeta}     & \code{pbeta}     & \code{qbeta}       & \code{rbeta}    \\
  \textbf{\code{Binomial}}         & \code{dbinom}    & \code{pbinom}    & \code{qbinom}      & \code{rbinom}   \\
  \textbf{\code{Cauchy}}           & \code{dcauchy}   & \code{pcauchy}   & \code{qcauchy}     & \code{rcauchy}  \\
  \textbf{\code{Chisq}}            & \code{dchisq}    & \code{pchisq}    & \code{qchisq}      & \code{rchisq}   \\
  \textbf{\code{Exponential}}      & \code{dexp}      & \code{pexp}      & \code{qexp}        & \code{rexp}     \\
  \textbf{\code{Fdist}}            & \code{df}        & \code{pf}        & \code{qf}          & \code{rf}       \\
  \textbf{\code{Gamma}}            & \code{dgamma}    & \code{pgamma}    & \code{qgamma}      & \code{rgamma}   \\
  \textbf{\code{Geometric}}        & \code{dgeom}     & \code{pgeom}     & \code{qgeom}       & \code{rgeom}    \\
  \textbf{\code{Hypergeometric}}   & \code{dhyper}    & \code{phyper}    & \code{qhyper}      & \code{rhyper}   \\
  \textbf{\code{LogNormal}}        & \code{dlnorm}    & \code{plnorm}    & \code{qlnorm}      & \code{rlnorm}   \\
  \textbf{\code{Multinomial}}      & \code{dmultinom} & \code{pmultinom} & \code{qmultinom}   & \code{rmultinom}\\
  \textbf{\code{NegativeBinomial}} & \code{dnbinom}   & \code{pnbinom}   & \code{qnbinom}     & \code{rnbinom}  \\
  \textbf{\code{Normal}}           & \code{dnorm}     & \code{pnorm}     & \code{qnorm}       & \code{rnorm}    \\
  \textbf{\code{Poisson}}          & \code{dpois}     & \code{ppois}     & \code{qpois}       & \code{rpois}    \\
  \textbf{\code{TDist}}            & \code{dt}        & \code{pt}        & \code{qt}          & \code{rt}       \\
  \textbf{\code{Uniform}}          & \code{dunif}     & \code{punif}     & \code{qunif}       & \code{runif}    \\
  \textbf{\code{Weibull}}          & \code{dweibull}  & \code{pweibull}  & \code{qweibull}    & \code{rweibull} \\
\end{tabular}
}
\label{statsfunctions}
\caption{List of common distributions in R v3.1.1\cite{rlang} and their
associated functions. Shown in bold are the equivalent Julia generic
functions and \code{Distribution} types provided by the Julia package
\package{Distributions.jl}, showing how the namespace
is simplified by generic functions and multiple dispatch.}
\end{table}

\TODO{show code examples}

\TODO{Other possible examples from Distributions.jl: QQ plots, posterior function}

\subsection{External dispatch with dynamic multimethods}

\paragraph{writemime example}
One thing you can do with dynamic multiple dispatch is external dispatch,
i.e.\ to be able to add new methods to an existing type rather than
specifying all the possible methods upfront. This is used to good effect
in Julia packages to extend functionality of the base library. For example,
the package \package{Color.jl} for manipulating color spaces extends the
\code{writemime} base function, which writes an object as a specified
MIME type\cite{mimerfc} to an I/O stream. \code{Color.jl} extends
\code{writemime} with new methods that draw color swatches.

\TODO{Here, we could put a figure showing writemime being called on a base
Julia object, writemime(::Color), and writemime(::VectorColor) objects.
Possibly take a screenshot in IJulia.}

\paragraph{External dispatch for package interaction}
External dispatch also facilitates interaction between Julia packages---
colors can be rendered using the \package{Cairo.jl} package,
which allows rendering to a Cairo backend~\cite{cairographics}.
When using Cairo to render an object in a given color, \code{Cairo.jl}
doesn't need to know anything about the underlying color space; it just
needs to call \code{convert} methods provided by \code{Color.jl}.
Julia's \code{convert} protocol is the ``narrow waist'' that allows
packages to allow using all kinds of color spaces naturally, without
needing to know anything specifically about them.

\paragraph{External dispatch is clunky in C++}
External dispatch can be performed in other languages like C++. However,
C++ only allows static external dispatch, which severely limits its utility.
Furthermore, external dispatch in C++ requires virtual methods and visitor
patterns\cite{designpatterns} to work around the absence of explicit support
for external dispatch.



\section{The Julia Language}
\TODO{Give an overview of the Julia language.}

We have designed the type system to be as permissive as possible to programmers
while still providing enough information to statically resolve a significant
portion of dispatches. The main contribution of the type system design is that
types are values that the programmer can write programs to compute. The
challenge in designing such a type system are in specifying how tags can be
computed and how the subtyping relationships work. \TODO{Question: is this type
system based on/similar to any other type system?} Motivating reason: having
types as values allows you to have fewer components to the language design.
Lets you write array parametric on a type without explicitly declaring the type
parameter.

Main challenges:
- Types/tags exist to describe things in a standard way. Challenge to think
about what needs to be described. Canonical example for long time was arrays
with different element types.
\TODO{What were the main difficulties in designing the type system?}
\TODO{What about promoters? Do type promotions show up in other languages?}
Type promotions are written as user-level code to compute new types.

\subsection{Types in Julia}
The defining feature of dynamic typing is that all values, semantically at
least, have two parts: a \emph{tag} and some \emph{data}. The tag classifies
the value according to some ontology defined by the programming language, and
the data is a block of memory whose format is set by the programming language,
possibly in a way that depends on the tag.

\paragraph{Our types are actually tags}
Colloquially tags are typically called ``types'' by programmers, as the distinction
is not important in most uses of dynamic typing. In type theory each tag corresponds
one-to-one to a type encoding the proposition that some term evaluates to a value
with that tag. However dynamically-typed languages usually do not require that such
a type system be used. Statically determining all tags is generally not possible.
Furthermore, it would be perfectly reasonable to impose a static type system that
was not concerned with tags at all, but rather with other program properties
(e.g. checking for possible uses of null references). Tags are a mechanism, and as
such neither require nor preclude any particular formalism.

\paragraph{Tags are constraining and hecne self-describing}
Tags are valuable because they have a constrained structure. While the data
part of a value may vary arbitrarily at run time, tags are drawn from a limited,
well-understood family. This provides for self-describing data: it becomes possible
to write a program that accepts an \emph{arbitrary} value, discovers its structure,
and operates on it, by examining its tag.

\paragraph{Tags incur overhead and most languages try to limit use of tags}
The potential of tags is to provide a common descriptive language shared by all users
of a language, as well as the compiler. The cost of tags is overhead. The addition of
a tag may double, or more, the memory footprint of a data item. Partly for this reason,
most dynamically typed languages try to simplify and minimize their tag systems.
\TODO{Laurence Tratt also points out that tags tend to resemble the types used in static
languages, likely as a result of cultural expectations.} Taken to an extreme,
as in Scheme, the set of possible tags might be finite and small, allowing tags to be
overlaid with pointer bits in many implementations.

\paragraph{Maximally expressive types in Julia}
Julia tries to reach the other extreme, providing for tags with nested structure, and
possibly containing arbitrary values.

- describing when code is applicable (dispatch)
- describing what to specialize on
- describing memory layout
- describing what, if anything, is statically known about a potential value

\paragraph{Type inference is a key part of the language}
A somewhat unusual feature of julia is that we consider dataflow type inference a
key part of the language. Strictly speaking, this is an optional, external program
analysis that might be used for various purposes, chief among them implementing
an optimizing compiler. However, it is highly important to programmers since it
largely defines not the semantics, but the performance model of the language.

\paragraph{Dataflow captures intuition of control flow}
We feel that dataflow analysis, especially of forward flow, captures a piece of
the human intuition of how programs work: values start at the top and move through
the program step by step. For example, compilers are much more user-friendly
when they elide a possibly-uninitialized variable warning in

\begin{minted}[frame=lines,framesep=2mm]{julia}
int a;
if (cond)
    a = 1;
else
    a = 2;
f(a);
\end{minted}

The programmer knows that \code{a} is always initialized before use.



\section{The Julia Type System}

\begin{quote}
  \textit{Ceci n'est pas une type} \\
  (With apologies to H. Magritte)
\end{quote}

\paragraph{Dynamic semantics only}
Julia has no static semantics, and therefore everything it does must be
dynamically describable.
In Julia all compiler values are pairs of a value v and a type tag T. Julia
values are essentially equivalent of what is sometimes called dynamic
types~\cite[Section 11.10, p. 142]{Pierce2002}

\paragraph{Subtyping relations and the type lattice}

Julia requires allows user-defined types to have exactly one declared supertype.
If no supertype is declared explicitly, the supertype is assumed to be \code{Any}.

The subtype relations define a lattice of types~\cite{Scott1976}. Furthermore
the lattice is bounded by a top type, \code{Any}, from above and a bottom
type, \code{Union()}, below.

We inherit the meet and join operations on the type lattice.

\paragraph{Kinds of types}
Julia has different kinds of types which can be formally summarized as:

\begin{minted}[frame=lines,framesep=2mm]{sml}
Type ::= Abstract | Data | Tuple | ForAll | Union | Singleton
Abstract ::= Name (P) Super }
Data ::= Name (P) Super Repr } invariant, nominative
TODO: What is repr?
Tuple ::= (T1, T2, …) | (T1, T2, …, Tn, …) } covariant
ForAll ::= $\forall$ (lb <: T <: ub) . Type
Union ::= U (T1, T2, …)
Singleton ::= Lift Value
TypeVar ::= lb <: Name <: ub
top ::= Any
--
Tag ::= Data | Tuple
\end{minted}

\begin{description}

\item[Data constructor/Tag types] (Pierce calls these atomic types or base types \cite[Fig.
	11-1, p.118]{Pierce2002}): types whose internal structure are not
	represented by the type system. (ACTUALLY I don't think this is true
	anymore, since \code{abstract} has no internal structure.) Julia has a
	special kind of tag type called \code{bitstype}s which simply label a
	contiguous number of bits.

??? Are things defined by \code{type} and \code{immutable} also tag types?
This is a different kind of type - they define an internal representation but
they are also abstract data types. These are \textit{abstract data types} that
completely define a different kind of type algebra and method interaction interface.
(These types abstract away the semantics of the containing fields and are not
record types.)

\code{DataType}s have associated inner and outer constructors.

\item[Tuple types] \code{(T1, T2,...)} which are constructed from the Cartesian
product of zero or more existing types \code{T1}, \code{T2}, etc.~\cite[Sec. 11.7]{Pierce2002}
Tuples are covariant.

\item[Abstract types] define a type with no declared representation. Abstract
	types are uninstantiable in the sense that there are no values whose
	types are abstract types. Their primary purpose is as declared
	supertypes of leaf types (which are instantiable); their role in
	providing structure in the type lattice is useful for writing
	multimethod signatures and type inference.

\item[Singleton types] In Julia, singleton types may be specified explicitly as
	using the \code{::Type\{T\}} construct. Singletons are used for
	computations on types themselves, such as type promotions. For example,
	given two types, can ask what the promoted type is. So far haven't had
	singleton types for general values. (Sorting algorithms example: used
	more for dispatch as shorthand for type with no parameters.) Will use
	this more because generating specialized code is big for scientific
	computing, but need to figure out what to specialize on and how to tell
	system what to specialize on. State of the art is to have ad hoc
	systems.

\item[Union types] declared as \code{Union(T1, T2,...)} which can be defined as
	the join of zero, or two or more, types \code{T1}, \code{T2},
	...~\cite[Sec.  15.7]{Pierce2002}
	%
	\begin{equation}
		Union(T_1, T_2, ..., T_N) = \bigwedge_{i=1}^N T_i 
	\end{equation}
	%
	The union with zero types, \code{Union()}, is simply the bottom type.
	%
	Julia's \code{Union}s are untagged and disjoint, which allows for some
	simplifications in their construction:
	\begin{itemize}
		\item If there are types \code{S} and \code{T} obeying the
			subtyping relation \code{S <: T}, then \code{S} is
			deleted from the final union type constructed. Note
			that this simplification includes the special case
			where \code{S} and \code T are identical.  (I think
			this is a $\beta$-reduction.)

		\item A union type \code{Union(T)} with a single type parameter
			\code{T} is identical to just \code{T}. (I think this
			is an $\eta$-reduction.)
	\end{itemize}

\item[\code{TypeVar}] A quantification over a set of types.  ForAll types
	quantify over all types between a lower bound and upper bound. Have
	lower bound and upper bound because of containers that can be read and
	written. People mostly only use the upper bounds. No function types, so
	no contravariance, so lower bounds don't get used much. In theory, can
	still use them for functions that mutate things.

\end{description}

\paragraph{Type parameters}

Parametric types are invariant.
The conventional wisdom is that type safety requires covariance if components
are read but not written, and contravariance if they are written but not
read~\cite{Castagna1995}. As type parameters can represent the types of mutable
fields in a \verb|type| which can be read or written, then the only safe choice
is invariance.

\paragraph{Example of user-definable type}

Example: the \code{Complex} type defined by

\begin{minted}{julia}
immutable Complex{T<:Real} <: Number
    re :: T
    im :: T
end
\end{minted}

has two declared fields, \code{re} and \code{im}. Each of these fields has the
type annotation \code{::T}. The type parameter \code{T} is constrained to be a
subtype of \code{Real}, and \code{Complex} itself has a declared supertype
which is \code{Number}.

\code{Complex} comes with a default, implicitly defined inner constructor

\mint{julia}|Complex{T<:Number}(re::T, y::T) = new(re, im)|
%
which makes use of a special intrinsic function \code{new} to allocate new
memory for the fields \code{re} and \code{im}. Inner constructors can be
defined by the user, in which case the default inner constructor is not
generated.

An instance of \code{Complex} can be created using the inner constructor, e.g.\

\mint{julia}|z = Complex{Float64}(0.0, 1.0)|

whose fields can be accessed directly as \code{P.x} and \code{P.y} respectively~
\footnote{\code{P.x} is desugared into the equivalent Julia code \code{getfield(P, x)}.
\code{getfield} is Julia's field accessor function.}

\paragraph{Nonobvious consequences}

A nonobvious consequence of the invariance of type parameters is that
parametric types can be instantiated with any type parameter regardless of
whether it is abstract or concrete.

The three \code{Complex} numbers

\begin{minted}{julia}
z1 = Complex{Float64}(0.0, 1.0)
z2 = Complex{FloatingPoint}(0.0, 1.0)
z3 = Complex{Union(Rational, Integer, Float64)}(0, 1.0)
\end{minted}

are all numerically equivalent (i.e.\ \code{z1 == z2 == z3}), but they are not
identical (in the \textit{egal} sense).

If we inspect the variables using Julia's \code{dump} command, we get:

\begin{minted}{jlcon}
julia> dump(z1)
Complex{Float64} 
  re: Float64 0.0
  im: Float64 1.0

julia> dump(z2)
Complex{FloatingPoint} 
  re: Float64 0.0
  im: Float64 1.0

julia> dump(z3)
Complex{Union(Integer,Float64,Rational{T<:Integer})} 
  re: Int64 0
  im: Float64 1.0

\end{minted}

We see that the \code{::T} annotation on a field does not guarantee that the
type of a value placed in the field in any given instance of that type is
exactly of type \code{T}, merely that it is a subtype of \code{T}.

\paragraph{abstract type}
An abstract type declares a type without a representation. It takes the name of
a type, parameters [what do the parameters do?], and the name of the super
type. Data type declarations additionally provide a representation. \TODO{Talk
about the representation.} Abstract type and data type declarations are
invariant and nominative.

\subsection{Basics}

\TODO{This is where we write one of those trees.}

Can write procedures to subtype.

Jeff is pretty sure subtyping is decidable and well-defined
Join is union of any two types; meet is less well-defined in this lattice

How does types as value play into this description?
What are types in Julia?
Dynamically typed.
What is inheritance like?
How does Julia avoid problems that OO languages tend to have with dispatch? (I'm not actually sure what they are.)
Is there multiple inheritance?
No, but it's been discussed (https://github.com/JuliaLang/julia/issues/5)
Is there multiple subtyping?
No, each type has only one supertype
We should also describe the dispatch mechanism.
Method tables are a sorted list of these types.
Implemented as \code{jl\_methtable\_t}


\paragraph{Code transformations in the Julia compiler}
The Julia code is parsed, then macro expanded, then lowered into SSA form, then
type inference runs on the lowered code where the code paths are essentially
linearized with jumps. This form is relatively straightforward to map onto LLVM
IR and is also simpler to run dataflow analysis on. After type inference comes
tuple elimination, inlining and then conversion to LLVM IR where further
optimizations passes occur.

\TODO{Is there a sense in which Julia "maximally leverages" LLVM?}

Type inference avoids explicit annotations and yet recovers almost enough information for performant dispatch. The goal is to dispatch onto machine types where possible and yet be flexible enough to support general code. Multiple dispatch was one part of the puzzle. The other part is type inference.

Type inference must be flow sensitive for a dynamic language since the type of a variable can change over the course of a program. Therefore we can't do Hindley Milner because it is flow insensitive. Instead, have to use dataflow-style static analysis for type inference.

Standard dataflow analysis makes use of the lattice structure of types to perform minimal fixed point analysis.

In general, type inference is NOT decidable! The set of pure terms which are typable in the λII-calculus in a given context is not recursive. So there is no general type inference algorithm for the programming language Elf and, in some cases, some type information has to be mentioned by the programmer. 

\paragraph{Dataflow analysis}
What is the goal of the dataflow analysis? Primarily for static type inference. Eliminate runtime checks. Lattice-based. Kaplan-Ullman.
Problem is finding the tightest possible set of allowable types.
There are tweaks to the standard algorithm. One big thing is that dataflow analysis only goes in forward direction.
Kaplan-Ullman also doesn't treat parametric types
Julia also has a widening step.
Relies on types having only one supertype.
Diagonal dispatch: can constrain arguments to same type.


\TODO{What are the challenges?}

\TODO{What is the complexity?}

\TODO{What is the best way to express this?}

\TODO{bit of background}

\paragraph{Abstract interpretation and flow functions}
The basic idea is abstract interpretation with flow functions (in Julia, called
t-functions) to represent the types of variables going in and out of each basic
block.  Abstract interpretation is essentially about systematic reasoning about
approximations. For type inference the approximation we make is to figure out
what types come in and out of program fragments. Further approximation is
widening which provides looser bounds on the inferred types. 

In general it suffices to consider lattices which have only finite chains. The
point of view of DFA based on abstract interpretation [CC77, AH87]is to replace
the standard semantics of programs by an abstract semantics describing how the
instructions operate on the abstract values A. Formally,we assume a
monotone semantic functional $![.!] : Instr \rightarrow (A \rightarrow A)$
which assigns a function on A to each instruction.

\paragraph{Mohnen's algorithm}
We use Mohnen's algorithm which avoids explicit initialization of graphical
representations of code structures, but instead work on the static single
assignment (SSA) form of the lowered code and has program counters. SSA is easy
to use because def-use chains tend to be very sparse (very few flow edges) and
there is exactly one definition reaching use of each variable, which reduces
the time needed to analyze. Mohnen's algorithm uses less memory and has better
data locality than standard graphical constructions like Tenenbaum? Muchnick?
etc, and furthermore avoids explicit transformations between lowered code, SA
graphs and BB graphs. Whereas Mohnen demonstrates its use on constant
propagation, we nevertheless use it for forward type inference, where the
computation at a node is influenced by information from previous nodes in the
control flow.

\paragraph{Existence and termination}
Monotonic flow functions (a.k.a.\ t-functions) are essential.
Monotonicity implies flow functions preserve the ordering of approximations.
parametric invariance makes it easy to construct monotonic flow functions since invariance means disjoint lattice structure
complete lattice with top implies that we have descending chains, so meets exists so a lub exists done.
It is also crucial for dataflow algorithms to converge to a fixed point
The finite height assumption of a lattice is essential to prove termination.

Parametric invariance makes it easy to construct monotone flow functions
because the underlying type lattice lends naturally to disentangled finite
chains and there is a lot more disjointedness to the lattice.

\paragraph{Subtyping relations and the type lattice}

\paragraph{Subtyping is not set embedding!}
Another aspect of the type system that can be confusing to new users is that
subtyping can express some, but not all, mathematical embeddings between sets
of allowed values.

Examples:

- Each 8-bit unsigned integer (Uint8) represents an integer in the set {0, 1,
\dots, 255}. Each of these in turn has an exact representation as a
double-precision floating point number (Float64). Nevertheless, Uint8 is not a
subtype of Float64.

- Int64 <: Real: each 64-bit signed integer (Int64) is a real number.

- Each value of \verb|Complex{Int64}| is a Gaussian integer, i.e. a complex number
where each component is an integer, and so each \verb|Complex{Int64}| is in 1:1
correspondence with a value in \verb|Complex{Integer}| and also in \verb|Complex{Real}|.
Nevertheless, \verb|Complex{Int64}| is a subtype of neither \verb|Complex{Integer}| not
\verb|Complex{Real}| due to parametric invariance. However, all of these are subtypes
of Complex.

- Real is not a subtype of Complex because for each type T that is a subtype of
Real, there is a corresponding instantiable type Complex{T}. None of these
correspond to Real since there is no T such that Complex{T} is a one-component
number, and each concrete type only has $\bot$ as its subtype.  


\paragraph{\code{typeof}}

You can define \code{typeof} axiomatically as having the following behavior on a \code{value=(bits, tag)} pair:

\begin{minted}[frame=lines,framesep=2mm]{julia}
typeof(bits, tag) = (tag, DataType) #Returns a value
\end{minted}

\code{typeof} has a fixed point, namely \code{(DataType, DataType)}. This is
also true in other dynamic languages, e.g.\ Python (CPython). In other
languages like Haskell, \code{typeof(DataType) = Kind}, etc. Static languages
can just truncate the tower of metatypes and also refuse to type-check code
that reasons about types and kinds too far up the hierarchy. In fact, early
versions of Haskell did not allow for programs to reason about kinds at the
data type level due to the lack of kind polymorphism~\cite{haskellkindtypes}.

\TODO{There is a subtlety about typeof's behavior. typeof is a projection;
typeof(not-a-type) produces a DataType, which projects non-type values onto
types. It also has the effect of lifting non-type values onto a type lattice;
the latter is defined only for values that are DataTypes.}

\paragraph{Widening for convergence acceleration}
We also use widening as a convergence acceleration technique. Although widening
does not guarantee converging to a minimal fixed point, the analysis
nonetheless converges and that is good enough.  Widening also deals with
infinite types because there are heuristic cutoffs for the traversal into the
substructure of data types.

\subsection{Dispatch}

\paragraph{method sorting for specificity}

\TODO{The main novelty and challenge is explaining type parameters and
typevars. Currently typevars are not first-class objects in Julia; you can't
pass them to a function. Expressions of the form \code{T<:SomeType} don't have
an independent existence outside a function signature that also contains the
\code{\{T...\}} construction.}

\subsection{Type inference}

\subsection{Type promotion}

\subsection{Example: modular integer arithmetic with \code{lcm}}

Here's an example of a type parameter computed with the \code{lcm} function:

\begin{minted}[frame=lines,fontsize=\footnotesize,
               framesep=2mm]{julia}
import Base: convert, promote_rule, show, showcompact

immutable ModInt{n} <: Integer
    k::Int
    ModInt(k) = new(mod(k,n))
end

-{n}(a::ModInt{n}) = ModInt{n}(-a.k)
+{n}(a::ModInt{n}, b::ModInt{n}) = ModInt{n}(a.k+b.k)
-{n}(a::ModInt{n}, b::ModInt{n}) = ModInt{n}(a.k-b.k)
*{n}(a::ModInt{n}, b::ModInt{n}) = ModInt{n}(a.k*b.k)

convert{n}(::Type{ModInt{n}}, k::Int) = ModInt{n}(k)
convert{n}(::Type{ModInt{n}}, k::ModInt) = ModInt{n}(k.k)
promote_rule{n}(::Type{ModInt{n}}, ::Type{Int}) = ModInt{n}
promote_rule{m,n}(::Type{ModInt{m}}, ::Type{ModInt{n}}) =
    ModInt{lcm(m,n)}

show{n}(io::IO, k::ModInt{n}) = print(io, "$(k.k) mod $n")
showcompact(io::IO, k::ModInt) = print(io, k.k)

julia> a = ModInt{12}(18278176231)
7 mod 12

julia> b = ModInt{15}(2837628736423)
13 mod 15

julia> a + b
20 mod 60
\end{minted}

The type of the result \code{a + b} depends on the types of \code{a} and \code{b} via \code{lcm}.

\section{Performance}

\TODO{Talk about how much type inference improves.}

\TODO{Talk about gains over non-Julia languages.}

\section{Dispatch in Practice}

\TODO{Talk about the library examples.}

\section{Discussion}

\TODO{Talk about the psychological implications of types.}


\section{Related Work}

Dynamic dispatch is more general than static dispatch since it covers cases
which cannot be determined at compile time.  Common examples involve data
retrieval with a persistent storage, generating string representations for text
output or data serialization.\cite{Shields1998}

\paragraph{Exotypes in Lua}
Exotypes\cite{exotypes} provide to the Lua language\cite{lua} the analogue of
macros that generate staged functions\cite{stagedfunc} in Julia.

There are some notable differences, though:

Exotypes are programmed in Terra,\cite{terra} a separate layer on top of the
Lua host language, and method specialization must be explicitly invoked from
within Terra. The syntax incurred by having two language layers creates an
artificial distinction between built-in methods and user-defined methods that
doesn't exist in Julia.

Exotypes implement automatic broadcasting when there is a
\code{\_\_methodmissing} property defined. Julia does not use automatic
broadcasting to implement the Proxy design pattern.

Julia uses an eager approach to resolving circularity in method definitions.
Exotypes use lazy evaluation, although interestingly, earlier versions of Terra
also used eager evaluation~\cite{terra}.

\TODO{Multiple dispatch paradigm may obviate the need for exotypes in some uses and make exotypes more powerful in others}

In a multiple dispatch paradigm, the issues brought up in Lua/Terra in wanting
to being able to create types with an unbounded number of behaviors becomes
irrelevant. There is a separation of functions and objects in Julia that
obviates the need for lazily queried properties (although it may still be a
more efficient implementation choice).

\paragraph{Dylan}

Like Julia, dispatch is dynamic, except where the compiler has determined
(possibly with sealing optimizations) that it can be optimized into a static
dispatch.

Dylan is object-oriented in the style of Common Lisp, but unlike Common Lisp,
also integrated the object system into the language.

Dylan supports a limited form of parametric types, which are called limited
types.\cite{dylanman} This allows for some modicum of generic programming.

Dylan allows structural inheritance from concrete types by structural
inheritance, i.e.\ by adding fields on top of those in the parent type. In
Julia all concrete types are final: inheritance is only allowed from abstract
types which are essentially what OO researchers call traits. Dylan lacks
traits, which are a useful mechanism for expressing generic programs.

Julia's focus is not on objects, but rather on generic and functional
programming.

Dylan supports multiple inheritance, while Julia does not and instead resorts
to duck typing.

An extension for Dylan allows for gradual typing, allowing for type information
to be determined at compile time~\cite{Mehnert2010}.

\paragraph{Dynamically-typed $\lambda$-calculus}

Several extensions of typed $\lambda$-calculus have been developed to handle
dynamic types.

One flavor is described in \cite{Henglein1994}, where a special type tag
\code{Dyn} was introduced as a placeholder type that is dynamically coerced
into other types. %But this is not the kind of polymorphism that Julia has

Other examples of coercive polymorphic $\lambda$-calculi: the $F^\uparrow_C$
language of \cite{Vytiniotis2012,Yorgey2012} used to describe the type
inference algorithm in GHC \cite{Weirich2011} contains type coercion
information that allows type errors to deferred to runtime
\cite{Vytiniotis2012} while preserving type-correctness. (This requires
so-called kind polymorphism, which allows for programs to reason about kinds.
\cite{Yorgey2012})

%Attempt to explain Julia's polymorphism features

However, this describes coercive polymorphism \cite{Cardelli1985}, which
is arguably not true polymorphism in the sense that it is not
universal~\cite{Strachey1967,Strachey2000}. In contrast, Julia supports two
distinct mechanisms for universal polymorphism, namely subtyping (a special
case of inclusion polymorphism), and parametric polymorphism.

Julia is not a true polymorphic system in the sense of \cite{Cardelli1985},
where code is generated only once for every generic procedure. Julia generates
a variety of specialized methods and so are more in the send of Ada-style
generic procedures which are abbreviations for sets of monomorphic procedures.

%Staged functions, dynamic λ-calculus, gradual typing... all seem to be closely related aspects of the same thing: it is often advantageous to split the determination of type information between compile time and runtime.

\paragraph{Haskell}

The main strategy in Haskell to deal with types that are not statically
decidable is to use staged type inference.~\cite{Shields1998}

\TODO{
Dynamic typing in polymorphic languages
M. Abadi, L. Cardelli, B. Pierce and D. Rémy Journal of Functional Programming / Volume 5 / Issue 01 / January 1995, pp 111 - 130 DOI: 10.1017/S095679680000126X
}

\section{Conclusions}

\paragraph{Logical challenges of parametric invariance}
Invariance is useful for things like arrays but they can make mathematical
reasoning tricky when the parametric type is something like complex.  Subtyping
is not embedding.

\paragraph{Parametric invariance interacts with dataflow in nonobvious ways}
Parametric invariance makes it easy to construct monotone flow functions
because the underlying type lattice lends naturally to disentangled finite
chains and there is a lot more disjoint functions.

The 1,2,Nat example and the induced structures on \verb|S{#} <: T{#}| make for
interesting flow functions.

Consider the flow function \verb|# -> S{#}|.

It is not distributive.
flow functions need not be distributive. Distribution is desirable because it
guarantees that merging information before function application does not result
in loss of precision. However the presence of types with invariant parameters
leads naturally to consider flow functions that are NOT distributive.

\paragraph{Recursive functions are problematic to infer}

\paragraph{Widening heuristics are challenging to design}
It is not obvious how to choose widening heuristics. Widening is NOT
commutative with arbitrary flow function. This example is a direct
counterexample to the intuition that widening (pessimizing) after analysis
produces the same result as analyzing after pessimizing. In fact, in this case
we cannot say that one has a subtype relation to the other, so that in general
switching the order of flow function application and widening can produce
different results which are neither tighter nor narrower approximations, but
merely different in general.

\paragraph{Type inference on infinite lattice}
Hard to do type inference on infinite lattice. We have infinities because of
typevars and infinite tuples.  In practice there are heuristics built into type
inference which trigger widening (pessimizing), such as maximum lengths of
unions and tuples, and the maximal depths of types and tuples analyzed.
 

\paragraph{Many unexplored implementation challenges}
Possibly peephole optimizations. Constant propagation. Backward type inference.
More convoluted thingies with a general abstract interpretation framework.



\acks{We thank the many Julia users and developers for their contributions to
the Julia language.}

\listoftodos %TODO remove from final submission

\bibliographystyle{abbrvnat}
\bibliography{pldi2015,websites}

\end{document}
